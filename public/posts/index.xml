<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Perspective Tensor</title>
    <link>https://entropy-warrior.github.io/posts/</link>
    <description>Recent content in Posts on Perspective Tensor</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 29 Jun 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://entropy-warrior.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Complextropy and Complexodynamics - Sutskever Reading List Series (1 of 30)</title>
      <link>https://entropy-warrior.github.io/posts/complexodynamics/</link>
      <pubDate>Sun, 29 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://entropy-warrior.github.io/posts/complexodynamics/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://entropy-warrior.github.io/images/blackhole.png&#34; alt=&#34;banner&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;What exactly is complexity? What is there to compress? I&amp;rsquo;ve been preoccupied with these ideas recently, especially after exploring Ilya Sutskever&amp;rsquo;s highly recommended top 30 ML reading list. This aligns closely with my writing about compression. It&amp;rsquo;s just another way of looking at how to model complex yet intelligent systems â€” a.k.a. creating ML models or &amp;ldquo;AI,&amp;rdquo; a term that&amp;rsquo;s overused these days.&lt;/p&gt;&#xA;&lt;p&gt;Anyway, let&amp;rsquo;s start with a very &amp;ldquo;simple&amp;rdquo; concept â€” entropy. You might have noticed it in my handle, &amp;ldquo;entropy warrior.&amp;rdquo; What exactly does entropy mean? You can easily find a definition with a quick Google search. Here&amp;rsquo;s what the latest Google AI search function tells us:&lt;/p&gt;</description>
    </item>
    <item>
      <title>ðŸ‘‹ Welcome to Lin&#39;s blog!</title>
      <link>https://entropy-warrior.github.io/posts/landing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://entropy-warrior.github.io/posts/landing/</guid>
      <description>&lt;p&gt;I hope you enjoy your visit. This blog is all about my journey of lifelong learning and shifting perspectives. As an &amp;ldquo;entropy warrior&amp;rdquo;, I love finding order and meaning in the chaos. Dive in with me as I explore data science, AI, and career growth, using first principles to see the world in new ways.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From Bird Flocks to Intelligence: The Power of Emergence</title>
      <link>https://entropy-warrior.github.io/posts/emergence/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://entropy-warrior.github.io/posts/emergence/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://entropy-warrior.github.io/images/emergence.png&#34; alt=&#34;bird flock&#34;&gt;&#xA;&lt;em&gt;(An image from Xavi Bou&amp;rsquo;s project Ornitographies.)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Have you ever found bird flocks mesmerizing? I certainly do. But did you know that it&amp;rsquo;s not the birds that memorize, but the flock itself? This behavior arises from simple rules and interactions. It&amp;rsquo;s fascinating how a system with simple components and rules can create something incredibly complex and sophisticated. I find this theme absolutely captivating and wanted to share it with you.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Life Lessons from Machine Learning</title>
      <link>https://entropy-warrior.github.io/posts/lifelesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://entropy-warrior.github.io/posts/lifelesson/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://entropy-warrior.github.io/images/lifelesson.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Lately, while diving into the latest and newest techniques in the world of AI, I couldn&amp;rsquo;t help but notice a few concepts that bear striking similarities to life lessons we learn as humans.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Reinforced Learning with Human Feedback (a.k.a. RLHF)&lt;/strong&gt;: In AI, this technique fine-tunes LLMâ€™s performance based on human feedback. In real life, as we grow through school and society, our belief system is constantly fine-tuned. The people around us shape our worldviews.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Modeling and Compression</title>
      <link>https://entropy-warrior.github.io/posts/compression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://entropy-warrior.github.io/posts/compression/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://entropy-warrior.github.io/images/blackhole.png&#34; alt=&#34;banner&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;think-your-model-is-smart-its-actually-just-really-good-at-compression&#34;&gt;Think Your Model is Smart? It&amp;rsquo;s Actually Just Really Good at Compression&lt;/h2&gt;&#xA;&lt;p&gt;Data scientists are the modern-day wizards, conjuring insights from chaos. But have you ever considered that these magical models are essentially performing a high-tech form of data compression?&lt;/p&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s break it down.&lt;/p&gt;&#xA;&lt;h3 id=&#34;models-the-executive-summary-of-reality&#34;&gt;Models: The Executive Summary of Reality&lt;/h3&gt;&#xA;&lt;p&gt;Imagine cramming a semester&amp;rsquo;s worth of history into a bite-sized executive summary. That&amp;rsquo;s the essence of modeling in the world of data science. We take complex and messy datasets and distill them down to the key ingredients that infer the outcome we care about. It&amp;rsquo;s like creating a newsletter for an event, capturing the essential points without the fluff.&lt;/p&gt;</description>
    </item>
    <item>
      <title>State Space Models vs Transformers: A Nuanced Perspective on Sequence Modeling</title>
      <link>https://entropy-warrior.github.io/posts/mamba/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://entropy-warrior.github.io/posts/mamba/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://entropy-warrior.github.io/images/SSM.png&#34; alt=&#34;mamba&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Have you ever wondered how AI models make sense of sequential data like text, time series, or medical images as time series? Beside the &amp;ldquo;traditional&amp;rdquo; recurrent neural network, two powerful machine learning architectures recently emerged as frontrunners in this field: transformers and State Space Models (SSM). But what sets them apart, and how do they compare? Let&amp;rsquo;s delve into sequence modeling and examine the nuanced differences between these two &amp;ldquo;newcomers&amp;rdquo;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>

<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>State Space Models vs Transformers: A Nuanced Perspective on Sequence Modeling | Perspective Tensor</title>
<meta name="keywords" content="">
<meta name="description" content="

Have you ever wondered how AI models make sense of sequential data like text, time series, or medical images as time series? Beside the &ldquo;traditional&rdquo; recurrent neural network, two powerful machine learning architectures recently emerged as frontrunners in this field: transformers and State Space Models (SSM). But what sets them apart, and how do they compare? Let&rsquo;s delve into sequence modeling and examine the nuanced differences between these two &ldquo;newcomers&rdquo;.">
<meta name="author" content="Lin Wang">
<link rel="canonical" href="https://entropy-warrior.github.io/posts/mamba/">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
<link crossorigin="anonymous" href="/assets/css/stylesheet.c5de734fbd88c3d21543485ffbcb1ccdda89a86a780cf987fa00199c41dbc947.css" integrity="sha256-xd5zT72Iw9IVQ0hf&#43;8sczdqJqGp4DPmH&#43;gAZnEHbyUc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://entropy-warrior.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://entropy-warrior.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://entropy-warrior.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://entropy-warrior.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://entropy-warrior.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://entropy-warrior.github.io/posts/mamba/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="State Space Models vs Transformers: A Nuanced Perspective on Sequence Modeling" />
<meta property="og:description" content="

Have you ever wondered how AI models make sense of sequential data like text, time series, or medical images as time series? Beside the &ldquo;traditional&rdquo; recurrent neural network, two powerful machine learning architectures recently emerged as frontrunners in this field: transformers and State Space Models (SSM). But what sets them apart, and how do they compare? Let&rsquo;s delve into sequence modeling and examine the nuanced differences between these two &ldquo;newcomers&rdquo;." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://entropy-warrior.github.io/posts/mamba/" /><meta property="article:section" content="posts" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="State Space Models vs Transformers: A Nuanced Perspective on Sequence Modeling"/>
<meta name="twitter:description" content="

Have you ever wondered how AI models make sense of sequential data like text, time series, or medical images as time series? Beside the &ldquo;traditional&rdquo; recurrent neural network, two powerful machine learning architectures recently emerged as frontrunners in this field: transformers and State Space Models (SSM). But what sets them apart, and how do they compare? Let&rsquo;s delve into sequence modeling and examine the nuanced differences between these two &ldquo;newcomers&rdquo;."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://entropy-warrior.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "State Space Models vs Transformers: A Nuanced Perspective on Sequence Modeling",
      "item": "https://entropy-warrior.github.io/posts/mamba/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "State Space Models vs Transformers: A Nuanced Perspective on Sequence Modeling",
  "name": "State Space Models vs Transformers: A Nuanced Perspective on Sequence Modeling",
  "description": " Have you ever wondered how AI models make sense of sequential data like text, time series, or medical images as time series? Beside the \u0026ldquo;traditional\u0026rdquo; recurrent neural network, two powerful machine learning architectures recently emerged as frontrunners in this field: transformers and State Space Models (SSM). But what sets them apart, and how do they compare? Let\u0026rsquo;s delve into sequence modeling and examine the nuanced differences between these two \u0026ldquo;newcomers\u0026rdquo;.\n",
  "keywords": [
    
  ],
  "articleBody": " Have you ever wondered how AI models make sense of sequential data like text, time series, or medical images as time series? Beside the “traditional” recurrent neural network, two powerful machine learning architectures recently emerged as frontrunners in this field: transformers and State Space Models (SSM). But what sets them apart, and how do they compare? Let’s delve into sequence modeling and examine the nuanced differences between these two “newcomers”.\nRise of Transformers: Attention is All You Need\nIn 2017, Transformers burst onto the scene by revolutionizing natural language processing. Their main innovation is the attention mechanism, which enables each element in a sequence to interact with every other element, thereby mastering the contextual connections within the sequence. In the transformer architecture, each element follows simple rules based on its relationship with all other elements. Language models such as GPT are built on transformers, showcasing the transformer architecture’s strength in handling highly contextual language.\nIn addition to excelling at tasks requiring global context understanding, Transformers have shown remarkable adaptability across various domains with surprising emergent capabilities such as reasoning and even math. However, their power and robustness come with a cost: computational complexity that scales quadratically with sequence length, limiting their efficiency on very long sequences.\nState Space Models: The Efficient Challengers\nWhile not new, state space models (SSMs) have recently surged in popularity for deep learning, particularly with innovations like the Mamba architecture. SSMs operate on a different principle: they maintain an internal state that evolves over time as they process a sequence.\nIn an SSM, each time step follows simple rules to update its state based on the previous state and new input. Collectively, these state transitions create a dynamic system capable of modeling complex temporal patterns. There’s no global view of the entire sequence simultaneously, but rather a continuously evolving representation.\nSSMs efficiently handle very long sequences and their computational complexity scales linearly with sequence length. This makes them particularly attractive for tasks involving extensive temporal dependencies or limited computational resources.\nEmerging Applications and Future Directions\nThe strengths and limitations of each approach have led to exciting developments in various domains. For instance, MedMamba, an SSM-based architecture, has shown promising results in medical image classification, offering competitive performance with fewer parameters and lower computational requirements.\nMeanwhile, Transformers continue to dominate in natural language processing tasks, leveraging their strong ability to handle context and perform information retrieval.\nLooking ahead, the most exciting prospects may lie in hybrid approaches that combine the strengths of both Transformers and SSMs. Researchers are exploring ways to improve SSMs’ performance on copying tasks and investigating hybrid architectures that could offer the best of both worlds.\nCitations:\nVaswani, A., et al. (2017). Attention is All You Need. Advances in Neural Information Processing Systems, 30, 5998-6008. Gu, A., et al. (2022). Efficiently Modeling Long Sequences with Structured State Spaces. International Conference on Learning Representations. Jelassi, S., et al. (2024). Repeat After Me: Transformers are Better than State Space Models at Copying. arXiv preprint arXiv:2402.01032v2. Yubian, Y., et al. (2024). MedMamba: Vision Mamba for Medical Image Classification. arXiv preprint arXiv:2403.03849. ",
  "wordCount" : "514",
  "inLanguage": "en",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Lin Wang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://entropy-warrior.github.io/posts/mamba/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Perspective Tensor",
    "logo": {
      "@type": "ImageObject",
      "url": "https://entropy-warrior.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://entropy-warrior.github.io/" accesskey="h" title="Perspective Tensor (Alt + H)">Perspective Tensor</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://entropy-warrior.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://entropy-warrior.github.io/codes/" title="Codes">
                    <span>Codes</span>
                </a>
            </li>
            <li>
                <a href="https://entropy-warrior.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/linwang4ds/" title="">
                    <span><i class="fab fa-linkedin"></i></span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://entropy-warrior.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://entropy-warrior.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      State Space Models vs Transformers: A Nuanced Perspective on Sequence Modeling
    </h1>
    <div class="post-meta">3 min&nbsp;·&nbsp;Lin Wang

</div>
  </header> 

  <div class="post-content"><p><img loading="lazy" src="/images/SSM.png" alt="mamba"  />
</p>
<p>Have you ever wondered how AI models make sense of sequential data like text, time series, or medical images as time series? Beside the &ldquo;traditional&rdquo; recurrent neural network, two powerful machine learning architectures recently emerged as frontrunners in this field: transformers and State Space Models (SSM). But what sets them apart, and how do they compare? Let&rsquo;s delve into sequence modeling and examine the nuanced differences between these two &ldquo;newcomers&rdquo;.</p>
<p><strong>Rise of Transformers: Attention is All You Need</strong></p>
<p>In 2017, Transformers burst onto the scene by revolutionizing natural language processing. Their main innovation is the attention mechanism, which enables each element in a sequence to interact with every other element, thereby mastering the contextual connections within the sequence. In the transformer architecture, each element follows simple rules based on its relationship with all other elements. Language models such as GPT are built on transformers, showcasing the transformer architecture&rsquo;s strength in handling highly contextual language.</p>
<p>In addition to excelling at tasks requiring global context understanding, Transformers have shown remarkable adaptability across various domains with surprising emergent capabilities such as reasoning and even math. However, their power and robustness come with a cost: computational complexity that scales quadratically with sequence length, limiting their efficiency on very long sequences.</p>
<p><strong>State Space Models: The Efficient Challengers</strong></p>
<p>While not new, state space models (SSMs) have recently surged in popularity for deep learning, particularly with innovations like the Mamba architecture. SSMs operate on a different principle: they maintain an internal state that evolves over time as they process a sequence.</p>
<p>In an SSM, each time step follows simple rules to update its state based on the previous state and new input. Collectively, these state transitions create a dynamic system capable of modeling complex temporal patterns. There&rsquo;s no global view of the entire sequence simultaneously, but rather a continuously evolving representation.</p>
<p>SSMs efficiently handle very long sequences and their computational complexity scales linearly with sequence length. This makes them particularly attractive for tasks involving extensive temporal dependencies or limited computational resources.</p>
<p><strong>Emerging Applications and Future Directions</strong></p>
<p>The strengths and limitations of each approach have led to exciting developments in various domains. For instance, MedMamba, an SSM-based architecture, has shown promising results in medical image classification, offering competitive performance with fewer parameters and lower computational requirements.</p>
<p>Meanwhile, Transformers continue to dominate in natural language processing tasks, leveraging their strong ability to handle context and perform information retrieval.</p>
<p>Looking ahead, the most exciting prospects may lie in hybrid approaches that combine the strengths of both Transformers and SSMs. Researchers are exploring ways to improve SSMs&rsquo; performance on copying tasks and investigating hybrid architectures that could offer the best of both worlds.</p>
<hr>
<p><strong>Citations:</strong></p>
<ol>
<li>Vaswani, A., et al. (2017). Attention is All You Need. Advances in Neural Information Processing Systems, 30, 5998-6008.</li>
<li>Gu, A., et al. (2022). Efficiently Modeling Long Sequences with Structured State Spaces. International Conference on Learning Representations.</li>
<li>Jelassi, S., et al. (2024). Repeat After Me: Transformers are Better than State Space Models at Copying. arXiv preprint arXiv:2402.01032v2.</li>
<li>Yubian, Y., et al. (2024). MedMamba: Vision Mamba for Medical Image Classification. arXiv preprint arXiv:2403.03849.</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://entropy-warrior.github.io/posts/compression/">
    <span class="title">« Prev</span>
    <br>
    <span>Modeling and Compression</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://entropy-warrior.github.io/">Perspective Tensor</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
